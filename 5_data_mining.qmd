---
title: "Metodi di Psicofisiologia e Neuroimaging nella Clinica e nelle Neuroscienze Cognitive"
subtitle: "Biomarker Data Mining"
author: "Raffaele M Mazziotti, PhD"
institute: "University of Florence"
resources:
  - quiz/
  - images/
format:
  revealjs:
    controls: true
    theme: default
    slide-number: true
    logo: images/logo/NEUROFARBA_logo.svg
    css: slide_style.css
    transition: fade
    center: true
    math: mathjax
    chalkboard: 
      buttons: false
    progress: true
    title-slide-attributes:
      data-background-image: images/logo/NEUROFARBA.svg
      data-background-size: 500px
      data-background-position: "center 10%"
      data-state: title-slide
    include-after-body: afterbody.html
    mermaid:
      theme: neutral
    
---

## What is Data Mining?

::: columns
:::column

Data mining is the process of:

- `Extracting patterns from data`  
- `Discovering structure` in complex datasets  
- Identifying predictive features  
- Transforming data into knowledge  

:::info-box
Modern datasets are:

- Multivariate
- High dimensional
- Non-linear
- Noisy
:::


:::
:::column

![](images/dm_data_mining.png){width="600" fig-align="center"}

:::
:::

## What Is a Dataset?

:::columns
:::column 

:::info-box
A dataset is:

`An organized collection of information`

In biomarker research, a dataset usually contains:

- participants  
- measurements  
- clinical labels  

It is typically organized as a `table`.
:::

:::
:::column

:::info-box
Most datasets are organized in rows and columns.

- Each `row` represents one participant  
- Each `column` represents one variable  
:::


|Participant | R T | Pupil Dilation | HbO | Diagnosis |  
|------------|---------------|----------------|---------------|-----------|  
|P01         | 520 ms        | 0.35 mm        | 1.2 µM        | Typical   | 
|P02         | 710 ms        | 0.62 mm        | 0.6 µM        | Disorder  | 


:::
:::

## Key Components of a Dataset

:::columns
:::column 

:::info-box
`Features`

- measurable variables  
- reaction time  
- EEG power  
- questionnaire scores  
:::

:::info-box
`Label` or `Target variable`

- what we want to predict  
- diagnosis  
- symptom severity  
- treatment response  
:::

:::
:::column 

![](images/dm_dataset_features.png){width="600" fig-align="center"}

```{mermaid}
flowchart TD

V["Statistical Variables"]

V --> C["Categorical (Qualitative)"]
V --> N["Numerical (Quantitative)"]

C --> CN["Nominal"]
C --> CO["Ordinal"]

N --> ND["Discrete"]
N --> NC["Continuous"]

```

:::
::: 

## What is Artificial Intelligence?

Artificial Intelligence (AI) refers to:

> Computational systems that learn patterns from data to make predictions or decisions.

::: columns
:::column

:::info-box
Artificial Intelligence is a broad field that includes:

- `Machine Learning` (ML): Algorithms learn patterns from data. 
- `Deep Learning`: Neural networks with many layers.
:::

::: 
:::column

![](images/dm_artificial_intelligence.png){width="600" fig-align="center"}

::: 
:::

## How AI Learns

::: columns
:::column

:::info-box
Most common `learning paradigms`:

[Supervised Learning]{.highlight}: 

Models are trained using labeled input-output pairs to predict outcomes for new data.

[Unsupervised Learning]{.highlight}:

Models that analyze and find hidden patterns, structures, or relationships in input data that is not labeled, categorized, or guided
:::

::: 
:::column

![](images/dm_supervised_unsupervised.png){width="350" fig-align="center"}

::: 
:::

## Supervised vs UnSupervised Learning

::: columns
:::column

:::info-box
In `Supervised` learning we provide:

- measurements (features) 
- diagnosis  (labels)

The system learns:

`How to predict diagnosis from measurements`

>This is the most relevant approach for biomarkers.

:::

::: 
:::column

:::info-box
In `UnSupervised` learning we provide:

- measurements  
- no diagnosis  

The system tries to:

- discover natural groups  
- identify hidden structure  

>Useful for discovering subgroups.

:::

::: 
:::

## Supervised vs UnSupervised Learning

![](images/dm_sup_unsup.png){width="350" fig-align="center"}

## What Is a Model?

::: columns
:::column

A model is:

A `rule` that combines `variables` to make predictions

:::card
Very simplified example:

`If`  
- reaction time is slow  
- pupil dilation is large  
- HbO is reduced  

`Then`  
probability of disorder increases.
:::

>The computer learns this rule automatically.


::: 
:::column

:::card
[Overfitting]{.highlight} occurs when a model learns training data too well, capturing `noise` and random fluctuations rather than just the underlying patterns.

How we prevent Overfitting?

Using `Cross-Validation`: testing the model on Data it has never seen before.
:::

![](images/dm_validation_test.jpg){width="600" fig-align="center"}

::: 
:::

## The Biomarker Discovery Pipeline

::: columns
:::column

1. `collect` data  
2. clean and `preprocess`  
3. `explore` variables  
4. `select relevant features`  
5. `train` predictive model  
6. `validate` on new data  
7. `interpret` carefully  
::: 
:::column

![](images/dm_classification.jpg){width="600" fig-align="center"}


::: 
:::

## Why Use Orange?

::: columns
:::column

`Orange` is:

- free  
- visual  
- easy to use  
- designed for learning  

>No coding required.


::: 
:::column

![](images/dm_orange.png){width="400" fig-align="center"}

<a href="https://orangedatamining.com/" target="_blank" class="btn">
Download
</a>

<a href="https://orangedatamining.com/docs/" target="_blank" class="btn">
Documentation
</a>


::: 
:::

## How Orange Works

::: columns
:::column

:::info-box
Orange uses visual blocks called `widgets`.

You connect them in a workflow.

Example:

Data → Model → Evaluation

>Each block performs one clear step.

:::

::: 
:::column

![](images/dm_orange_widgets.png){width="400" fig-align="center"}

::: 
:::

## Loading a Dataset

::: columns
:::column

:::info-box
The main widget is:

`CSV import`

It allows you to:

- open a Comma-Separated Value (CSV) file
- inspect variables  

After loading, the data can be sent to other widgets.

> For other file types you can also use the `File` widget

:::

::: 
:::column

![](images/dm_orange_csv.png){width="100" fig-align="left"}
![](images/dm_orange_csv_prompt.png){width="500" fig-align="left"}

::: 
:::

## Data Table

::: columns
:::column

:::info-box
The `Data Table` widget lets you:

- see all participants  
- inspect variables  
- check missing values  

This is your first contact with the dataset.
:::

::: 
:::column

![](images/dm_orange_data_table.png){width="100" fig-align="left"}

::: 
:::

## Select Columns

::: columns
:::column

:::info-box
The `Select Columns` widget lets you:

- choose which variables are used for prediction  
- define the `target variable`  
- assign descriptive information as `meta`  

This is where you clearly define:

`What do I want to predict, and from which variables?`
:::

::: 
:::column

![](images/dm_orange_select_columns.png){width="100" fig-align="left"}
![](images/dm_orange_select_columns_menu.png){width="500" fig-align="left"}

::: 
:::

## Impute

::: columns
:::column

:::info-box
`Impute`

- handle missing values  
:::

::: 
:::column

![](images/dm_orange_impute.png){width="80" fig-align="left"}
![](images/dm_orange_impute_menu.png){width="500" fig-align="left"}

::: 
:::

## Preprocess

::: columns
:::column

:::info-box
`Preprocess`

- Transforma data 
- Normalize, discretize, etc
:::


::: 
:::column
![](images/dm_orange_preprocess.png){width="100" fig-align="left"}
![](images/dm_orange_preprocess_menu.png){width="500" fig-align="left"}

::: 
:::

## Exploring the Data

::: columns
:::column

Visualization helps us understand patterns.

:::card
`Distributions`

- inspect variable distributions  
:::

:::card
`Box Plot`

- compare groups  
:::

:::card
`Scatter Plot`

- explore relationships  
:::

::: 
:::column

![](images/dm_orange_distr_bp_scatter.png){width="150" fig-align="left"}

::: 
:::

## Features Ranking

::: columns
:::column

:::info-box
The `Rank` widget evaluates how informative each variable is for predicting the target. It:

- scores each feature individually  
- orders variables from most to least informative
:::

:::info-box
One common scoring method is `Information Gain`. `Information Gain` measures:

- how much a variable reduces uncertainty ([entropy](https://en.wikipedia.org/wiki/Entropy_(information_theory))) about the diagnosis  
- how much knowing that variable improves classification
:::

::: 
:::column

![](images/dm_orange_rank.png){width="100" fig-align="left"}
![](images/dm_orange_menu.png){width="500" fig-align="left"}

::: 
:::

## Models: Logistic Regression

::: columns
:::column

::: info-box
The `Logistic Regression` widget is a simple and interpretable classification model to classify data into two groups.
It:

- estimates the probability of belonging to a group  
- combines variables linearly  
- works well when relationships are relatively simple  
:::

::: info-box
It is often a good first model to try because:

- it is stable  
- it is easy to interpret
:::

::: 
:::column


![](images/dm_orange_logistic_regression.png){width="100" fig-align="left"}
![](images/dm_logistic_regression.png){width="500" fig-align="left"}

::: 
:::

## Models: Random Forest

::: columns
:::column

The `Random Forest` widget is a more flexible and powerful model.

:::info-box
It:

- builds many decision trees  
- combines their predictions  
- handles complex and non-linear relationships  
:::

:::info-box
It is useful when:

- variables interact with each other  
- patterns are not simple  
- prediction `accuracy is the main goal` 
:::

::: 
:::column

![](images/dm_orange_random_forest.png){width="100" fig-align="left"}
![](images/dm_random_forest.png){width="500" fig-align="left"}

::: 
:::

## Models: Support Vector Machine

::: columns
:::column

:::info-box
The `Support Vector Machine` widget is a model that tries to find the best boundary between groups.

It:

- separates groups with an optimal decision boundary
- works well in high-dimensional data  
:::

::: info-box
It is often effective when:

- groups overlap  
- there are many variables  
- the separation is subtle  
:::

::: 
:::column

![](images/dm_orange_svm.png){width="100" fig-align="left"}
![](images/dm_support_vector_machine.webp){width="500" fig-align="left"}

::: 
:::

## Models: Neural Network

::: columns
:::column

:::info-box
The `Neural Network` widget is a model inspired by how biological neurons process information. It:

- combines variables through `multiple processing layers`  
- can learn complex and non-linear patterns  
- adjusts internal connections to improve prediction  
:::

:::info-box
Neural networks are useful when:

- relationships between variables are complex  
- patterns are not linear  

:::

::: 
:::column

:::info-box
However:

- they are less interpretable than simpler models
- they are more sensitive to overfitting  

They are powerful tools, but should be used carefully in clinical research.
:::

![](images/dm_orange_ANN.png){width="100" fig-align="left"}
![](images/dm_multilayeer_perceptron.png){width="350" fig-align="left"}

::: 
:::

## Test and Score

::: columns
:::column

:::info-box
The `Test and Score` widget evaluates how well a model performs. This step is essential to avoid `overfitting`. It:

- trains the model  
- tests it on unseen data  
- computes performance metrics

:::

:::info-box
The widget reports measures such as:

- accuracy  
- AUC  
- precision  
- recall  

:::

::: 
:::column

![](images/dm_orange_test_score.png){width="100" fig-align="left"}
![](images/dm_orange_test_score_menu.png){width="500" fig-align="left"}

::: 
:::

## Model Evaluation: Accuracy & Recall

::: columns
:::column

>`Accuracy` is the proportion of correctly classified individuals.

It answers:

`How often is the model correct overall?`

:::card
If accuracy is 0.80, the model correctly classifies 80 percent of participants.
:::


>[Important limitation]{.highlight}:
>If one group is much larger than the other,  
>accuracy can be misleading.

::: 
:::column

>`Recall`, reflects sensitivity.

`Of all the real disorder cases, how many did we correctly detect?`

:::card
Low recall means:

- many true cases were missed  
:::

>Clinically, this relates to `missed diagnoses`.


::: 
:::

## Model Evaluation: Precision & F1 Score

::: columns
:::column

`Precision` answers:

`Of all the cases predicted as disorder, how many are truly disorder?`

:::card
Low precision means:

- many false positives  
:::

>Clinically, this relates to false alarms.

::: 
:::column

>`F1 score` combines precision and recall into a single number.

:::card
It answers:

`Is the model good at detecting true cases while avoiding false alarms?`
:::

:::info-box
It is especially useful when:

- classes are imbalanced  
- both false positives and false negatives matter  
:::

::: 
:::

## Model Evaluation: AUC

::: columns
:::column

>`AUC` stands for [Area Under the Curve]{.highlight}.

:::card
It measures:

`How well the model separates the two groups overall`
:::

:::info-box
It ranges from:

- 0.5 → random guessing  
- 1.0 → perfect separation  

AUC is useful because:

- it is less sensitive to class imbalance  
- it reflects overall discriminative ability.
:::

::: 
:::column

![](images/dm_auc.png){width="500" fig-align="left"}

::: 
:::

## Model Evaluation: Why Multiple Metrics Matter

::: columns
:::column

>No single number tells the whole story.

:::info-box
A clinically useful model should:

- detect true cases  
- avoid false alarms  
- generalize to new patients  
:::

:::card
That is why we examine:

accuracy, recall, precision, F1 score, and AUC together.
:::

::: 
:::column

![](images/dm_metrics.png){width="400" fig-align="left"}

::: 
:::

## `Course Project`: Biomarker Discovery

::: columns
:::column

For this course you will complete an `individual project`.

:::info-box
You will receive:

- one dataset  
- one experiment description  
- one HTML file explaining the variables  
:::

>Your task is to analyze the dataset and propose potential biomarkers.

::: 
:::column


>Each student will receive a different `dataset`.

:::info-box
Each dataset contains:

- one experiment  
- two groups: `Typical` and `Atypical`  
- several measured variables  


Each row represents one participant.  
Each column represents one variable.

One column contains the group label.
:::

::: 
:::

## Step 1: Explore and Visualize

::: columns
:::column

:::info-box
Before building any model, you must:

- read the experiment description carefully  
- understand what each variable represents  
- identify the `target variable`  
:::

![](images/dm_data_visualization.jpg){width="350" fig-align="center"}

::: 
:::column

:::info-box
You must:

- check missing data
- plot all variables
- inspect distributions
- compare groups visually and statistically

Use:

- box plots  
- distributions  
- scatter plots  

Goal:

`Understand how Typical and Atypical subjects differ`
:::

::: 
:::

## Step 2: Identify Potential Biomarkers

::: columns
:::column

:::info-box
Using Orange:

- build classification models  
- test different algorithms  
- compare performances  
:::

:::info-box
For your best model, you must report:

- accuracy  
- precision  
- recall  
- F1 score  
- AUC  
:::

::: 
:::column


:::card
You must explain:

`What do these numbers mean in clinical terms?`

For example:

- Are there many false positives?  
- Are true cases being missed?
:::

:::info-box
You must try to improve performance by:

- prune variables using rank widget
- comparing models  

:::

::: 
:::

## Step 4: Final Deliverable

::: columns
:::column

:::info-box
>You will present your project in `7 minutes`.

Your presentation should include:

1. Brief description of the experiment
2. Exploratory analysis  
3. Group differences  
4. Best predictive model  
5. Performance metrics  
6. Interpretation and conclusions  

>Clarity is more important than complexity.

:::

::: 
:::column

:::info-box
Your project will be evaluated based on:

- Correct data exploration  
- Appropriate use of models  
- Proper interpretation of metrics   
- Scientific reasoning  
- Clarity of presentation  
:::

:::info-box
This is not about:

- building the most complex model  
- obtaining perfect accuracy  

:::

::: 
:::
